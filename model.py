import torch
import torch.nn as nn
import torch.nn.functional as F


class TextCNN(nn.Module):
	def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes):
		super(TextCNN, self).__init__()

		self.vocab_size = vocab_size
		self.embedding_dim = embedding_dim
		self.kernel_sizes = kernel_sizes
		self.num_filters = num_filters
		self.num_classes = num_classes
		self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)
		self.conv = nn.ModuleList([
			nn.Conv1d(
				in_channels = embedding_dim,
				out_channels = num_filters,
				kernel_size = k,
				stride = 1
			) for k in kernel_sizes])
		self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

	def forward(self, x):
		batch_size, sequence_length = x.shape
		x = self.embedding(x.T).transpose(1, 2)
		x = [F.relu(conv(x)) for conv in self.conv]
		x = [F.max_pool1d(c, c.size(-1)).squeeze(dim = -1) for c in x]
		x = torch.cat(x, dim = 1)
		x = self.fc(x)
		return x
