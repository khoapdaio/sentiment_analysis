{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importing necessary libraries for randomness, numerical computations, and PyTorch functionalities\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Setting a fixed seed to ensure reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Configuring PyTorch backend to make computations deterministic for CUDA\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "id": "e892169db12d71dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **CNN for Text Classification**",
   "id": "278342d9a60242f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Load dataset**"
   ],
   "id": "cbbb22a54aae1ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!git clone https://github.com/congnghia0609/ntc-scv.git",
   "id": "3f90508236f0414a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!unzip ./ntc-scv/data/data_test.zip -d ./data\n",
    "!unzip ./ntc-scv/data/data_train.zip -d ./data\n",
    "!rm -rf ./ntc-scv"
   ],
   "id": "65178d74e2a408ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_from_path(folder_path):\n",
    "\texamples = []\n",
    "\tfor label in os.listdir(folder_path):\n",
    "\t\tfull_path = os.path.join(folder_path, label)\n",
    "\t\tfor file_name in os.listdir(full_path):\n",
    "\t\t\tfile_path = os.path.join(full_path, file_name)\n",
    "\t\t\twith open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "\t\t\t\tlines = f.readlines()\n",
    "\t\t\tsentence = \" \".join(lines)\n",
    "\t\t\tif label == \"neg\":\n",
    "\t\t\t\tlabel = 0\n",
    "\t\t\tif label == \"pos\":\n",
    "\t\t\t\tlabel = 1\n",
    "\t\t\tdata = {\n",
    "\t\t\t\t'sentence': sentence,\n",
    "\t\t\t\t'label': label\n",
    "\t\t\t}\n",
    "\t\t\texamples.append(data)\n",
    "\treturn pd.DataFrame(examples)\n",
    "\n",
    "\n",
    "folder_paths = {\n",
    "\t'train': './data/data_train/train',\n",
    "\t'valid': './data/data_train/test',\n",
    "\t'test': './data/data_test/test'\n",
    "}\n",
    "\n",
    "train_df = load_data_from_path(folder_paths['train'])\n",
    "valid_df = load_data_from_path(folder_paths['valid'])\n",
    "test_df = load_data_from_path(folder_paths['test'])\n"
   ],
   "id": "75987b3767616a6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Preprocessing**",
   "id": "c573c245aac99e96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "\n",
    "def identify_vn(df):\n",
    "\tidentifier = LanguageIdentifier.from_modelstring(model, norm_probs = True)\n",
    "\tnot_vi_idx = set()\n",
    "\tTHRESHOLD = 0.9\n",
    "\tfor idx, row in df.iterrows():\n",
    "\t\tscore = identifier.classify(row[\"sentence\"])\n",
    "\t\tif score[0] != \"vi\" or (score[0] == \"vi\" and score[1] <= THRESHOLD):\n",
    "\t\t\tnot_vi_idx.add(idx)\n",
    "\tvi_df = df[~df.index.isin(not_vi_idx)]\n",
    "\tnot_vi_df = df[df.index.isin(not_vi_idx)]\n",
    "\treturn vi_df, not_vi_df\n",
    "\n",
    "\n",
    "train_df_vi, train_df_other = identify_vn(train_df)\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "\t# remove URLs https://www.\n",
    "\turl_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
    "\ttext = url_pattern.sub(r\" \", text)\n",
    "\n",
    "\t# remove HTML Tags: <>\n",
    "\thtml_pattern = re.compile(r'<[^<>]+>')\n",
    "\ttext = html_pattern.sub(\" \", text)\n",
    "\n",
    "\t# remove puncs and digits\n",
    "\treplace_chars = list(string.punctuation + string.digits)\n",
    "\tfor char in replace_chars:\n",
    "\t\ttext = text.replace(char, \" \")\n",
    "\n",
    "\t# remove emoji\n",
    "\temoji_pattern = re.compile(\"[\"\n",
    "\t                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "\t                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "\t                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "\t                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "\t                           u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "\t                           u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "\t                           u\"\\U0001F600-\\U0001F64F\"\n",
    "\t                           u\"\\U00002702-\\U000027B0\"\n",
    "\t                           u\"\\U000024C2-\\U0001F251\"\n",
    "\t                           u\"\\U0001f926-\\U0001f937\"\n",
    "\t                           u\"\\U0001F1F2\"\n",
    "\t                           u\"\\U0001F1F4\"\n",
    "\t                           u\"\\U0001F620\"\n",
    "\t                           u\"\\u200d\"\n",
    "\t                           u\"\\u2640-\\u2642\"\n",
    "\t                           \"]+\", flags = re.UNICODE)\n",
    "\ttext = emoji_pattern.sub(r\" \", text)\n",
    "\n",
    "\t# normalize whitespace\n",
    "\ttext = \" \".join(text.split())\n",
    "\n",
    "\t# lowercasing\n",
    "\ttext = text.lower()\n",
    "\treturn text\n",
    "\n",
    "\n",
    "train_df_vi['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in train_df_vi.iterrows()]\n",
    "valid_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in valid_df.iterrows()]\n",
    "test_df['preprocess_sentence'] = [preprocess_text(row['sentence']) for index, row in test_df.iterrows()]\n",
    "\n",
    "train_df_vi"
   ],
   "id": "b8f989ea34d4c9ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Representation**",
   "id": "7ea193106e71d88b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def yield_tokens(sentences, tokenizer):\n",
    "\tfor sentence in sentences:\n",
    "\t\tyield tokenizer(sentence)\n",
    "\n"
   ],
   "id": "17c0ea2ff03dc1a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# word-based tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n"
   ],
   "id": "8649411eb14f6094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# build vocabulary\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(\n",
    "\tyield_tokens(train_df_vi['preprocess_sentence'], tokenizer),\n",
    "\tmax_tokens = vocab_size,\n",
    "\tspecials = [\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "vocabulary[\"<pad>\"]\n"
   ],
   "id": "7e8adfcb9a5856b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "def prepare_dataset(df):\n",
    "\t# create iterator for dataset: (sentence, label)\n",
    "\tfor index, row in df.iterrows():\n",
    "\t\tsentence = row['preprocess_sentence']\n",
    "\t\tencoded_sentence = vocabulary(tokenizer(sentence))\n",
    "\t\tlabel = row['label']\n",
    "\t\tyield encoded_sentence, label\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(train_df_vi)\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "valid_dataset = to_map_style_dataset(valid_dataset)"
   ],
   "id": "612071b4c92292f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Dataloader**",
   "id": "815071a973ba734c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###**Dataloader**\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "\t# create inputs, offsets, labels for batch\n",
    "\tencoded_sentences, labels = [], []\n",
    "\tfor encoded_sentence, label in batch:\n",
    "\t\tlabels.append(label)\n",
    "\t\tencoded_sentence = torch.tensor(encoded_sentence, dtype = torch.int64)\n",
    "\t\tencoded_sentences.append(encoded_sentence)\n",
    "\n",
    "\tlabels = torch.tensor(labels, dtype = torch.int64)\n",
    "\tencoded_sentences = pad_sequence(\n",
    "\t\tencoded_sentences,\n",
    "\t\tpadding_value = vocabulary[\"<pad>\"]\n",
    "\t)\n",
    "\n",
    "\treturn encoded_sentences, labels\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size = batch_size,\n",
    "\tshuffle = True,\n",
    "\tcollate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "\tvalid_dataset,\n",
    "\tbatch_size = batch_size,\n",
    "\tshuffle = False,\n",
    "\tcollate_fn = collate_batch\n",
    ")\n",
    "\n",
    "next(iter(train_dataloader))\n",
    "\n",
    "encoded_sentences, labels = next(iter(train_dataloader))\n",
    "\n",
    "encoded_sentences.shape"
   ],
   "id": "360cfedfd4abafc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Model**",
   "id": "a9a06f5539cad308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###**Model**\n",
    "\n",
    "import torch.nn as nn\n",
    "from model import TextCNN\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 100\n",
    "\n",
    "model = TextCNN(\n",
    "\tvocab_size = vocab_size,\n",
    "\tembedding_dim = embedding_dim,\n",
    "\tkernel_sizes = [3, 4, 5],\n",
    "\tnum_filters = 100,\n",
    "\tnum_classes = 2\n",
    ")\n",
    "\n",
    "vocab_size\n",
    "\n"
   ],
   "id": "3808b4c352416261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions = model(encoded_sentences)\n",
    "predictions\n"
   ],
   "id": "40e71421eba14b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predictions.shape\n",
   "id": "fd70674847614ee6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Loss & Optimizer**",
   "id": "6d6c8ca89feb9049"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = criterion(predictions, labels)\n",
    "loss\n"
   ],
   "id": "6c3851c4a8f044c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Trainer**",
   "id": "354b6c44d433dd31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, device, epoch = 0, log_interval = 50):\n",
    "\tmodel.train()\n",
    "\ttotal_acc, total_count = 0, 0\n",
    "\tlosses = []\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t# zero grad\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# predictions\n",
    "\t\tpredictions = model(inputs)\n",
    "\n",
    "\t\t# compute loss\n",
    "\t\tloss = criterion(predictions, labels)\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t# backward\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "\t\ttotal_count += labels.size(0)\n",
    "\t\tif idx % log_interval == 0 and idx > 0:\n",
    "\t\t\telapsed = time.time() - start_time\n",
    "\t\t\tprint(\n",
    "\t\t\t\t\"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "\t\t\t\t\"| accuracy {:8.3f}\".format(\n",
    "\t\t\t\t\tepoch, idx, len(train_dataloader), total_acc / total_count\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\ttotal_acc, total_count = 0, 0\n",
    "\t\t\tstart_time = time.time()\n",
    "\n",
    "\tepoch_acc = total_acc / total_count\n",
    "\tepoch_loss = sum(losses) / len(losses)\n",
    "\treturn epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "epoch_acc, epoch_loss = train(model, optimizer, criterion, train_dataloader, device)\n",
    "\n",
    "epoch_acc, epoch_loss"
   ],
   "id": "352f474fe62cf014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def evaluate(model, criterion, valid_dataloader, device):\n",
    "\tmodel.eval()\n",
    "\ttotal_acc, total_count = 0, 0\n",
    "\tlosses = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor idx, (inputs, labels) in enumerate(valid_dataloader):\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\t\t\t# predictions\n",
    "\t\t\tpredictions = model(inputs)\n",
    "\n",
    "\t\t\t# compute loss\n",
    "\t\t\tloss = criterion(predictions, labels)\n",
    "\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t\ttotal_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "\t\t\ttotal_count += labels.size(0)\n",
    "\n",
    "\tepoch_acc = total_acc / total_count\n",
    "\tepoch_loss = sum(losses) / len(losses)\n",
    "\treturn epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader, device)\n",
    "\n",
    "eval_acc, eval_loss"
   ],
   "id": "fe2fc4aa3fc77a1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Training**",
   "id": "9231bae4a273a113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_class = 2\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 300\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextCNN(\n",
    "\tvocab_size = vocab_size,\n",
    "\tembedding_dim = embedding_dim,\n",
    "\tkernel_sizes = [3, 4, 5],\n",
    "\tnum_filters = 100,\n",
    "\tnum_classes = 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 2e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "save_model = './model'\n",
    "\n",
    "train_accs, train_losses = [], []\n",
    "eval_accs, eval_losses = [], []\n",
    "best_loss_eval = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\tepoch_start_time = time.time()\n",
    "\t# Training\n",
    "\ttrain_acc, train_loss = train(model, optimizer, criterion, train_dataloader, device, epoch)\n",
    "\ttrain_accs.append(train_acc)\n",
    "\ttrain_losses.append(train_loss)\n",
    "\n",
    "\t# Evaluation\n",
    "\teval_acc, eval_loss = evaluate(model, criterion, valid_dataloader, device)\n",
    "\teval_accs.append(eval_acc)\n",
    "\teval_losses.append(eval_loss)\n",
    "\n",
    "\t# Save best model\n",
    "\tif eval_loss < best_loss_eval:\n",
    "\t\tprint('Save model at ./model/text_cnn_model.pt')\n",
    "\t\ttorch.save(model.state_dict(), save_model + '/text_cnn_model.pt')\n",
    "\n",
    "\t# Print loss, acc end epoch\n",
    "\tprint(\"-\" * 59)\n",
    "\tprint(\n",
    "\t\t\"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
    "\t\t\"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
    "\t\t\tepoch, time.time() - epoch_start_time, train_acc, train_loss, eval_acc, eval_loss\n",
    "\t\t)\n",
    "\t)\n",
    "\tprint(\"-\" * 59)\n",
    "\n",
    "\t# Load best model\n",
    "\tmodel.load_state_dict(torch.load(save_model + '/text_cnn_model.pt'))\n",
    "\tmodel.eval()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
    "\tepochs = list(range(num_epochs))\n",
    "\tfig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 6))\n",
    "\taxs[0].plot(epochs, train_accs, label = \"Training\")\n",
    "\taxs[0].plot(epochs, eval_accs, label = \"Evaluation\")\n",
    "\taxs[1].plot(epochs, train_losses, label = \"Training\")\n",
    "\taxs[1].plot(epochs, eval_losses, label = \"Evaluation\")\n",
    "\taxs[0].set_xlabel(\"Epochs\")\n",
    "\taxs[1].set_xlabel(\"Epochs\")\n",
    "\taxs[0].set_ylabel(\"Accuracy\")\n",
    "\taxs[1].set_ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\n",
    "\n",
    "plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses)"
   ],
   "id": "f7ef93eaa641dc49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Evaluation & Prediction**",
   "id": "9b8e91bee7d4db1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "test_dataset = prepare_dataset(test_df)\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "\ttest_dataset,\n",
    "\tbatch_size = batch_size,\n",
    "\tshuffle = False,\n",
    "\tcollate_fn = collate_batch\n",
    ")\n",
    "\n",
    "test_acc, test_loss = evaluate(model, criterion, test_dataloader, device)\n",
    "test_acc, test_loss"
   ],
   "id": "6c9a16c96a723c4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Inference**",
   "id": "63cf32e11496fec1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def load_model(model_path, vocab_size = 10000, embedding_dim = 300, num_classes = 2):\n",
    "\tmodel = TextCNN(\n",
    "\t\tvocab_size = vocab_size,\n",
    "\t\tembedding_dim = embedding_dim,\n",
    "\t\tkernel_sizes = [3, 4, 5],\n",
    "\t\tnum_filters = 100,\n",
    "\t\tnum_classes = num_classes\n",
    "\t)\n",
    "\tmodel.load_state_dict(torch.load(model_path, weights_only = True))\n",
    "\tmodel.eval()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def inference(sentence, vocabulary, model):\n",
    "\tsentence = preprocess_text(sentence)\n",
    "\tencoded_sentence = vocabulary(tokenizer(sentence))\n",
    "\tencoded_sentence = torch.tensor(encoded_sentence)\n",
    "\tencoded_sentence = torch.unsqueeze(encoded_sentence, 1)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tpredictions = model(encoded_sentence)\n",
    "\tpreds = nn.Softmax(dim = 1)(predictions)\n",
    "\tp_max, yhat = torch.max(preds.data, 1)\n",
    "\treturn round(p_max.item(), 2) * 100, yhat.item()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), save_model + '/text_cnn_model.pt')\n",
   "id": "219b99e8746925bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model = load_model('./model/text_cnn_model.pt')\n",
    "inference(test_df['preprocess_sentence'][5999], vocabulary, model)\n",
    "\n",
    "test_df"
   ],
   "id": "5d245985ccfd661f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inference('Đồ ăn rất ngon và rẻ', vocabulary, model)\n",
    "\n",
    "test_df['preprocess_sentence'][5999]"
   ],
   "id": "99dfce411398d56f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
